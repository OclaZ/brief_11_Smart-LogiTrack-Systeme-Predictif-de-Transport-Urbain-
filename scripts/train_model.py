# scripts/train_model.py
from pyspark.sql import SparkSession
from pyspark.ml.feature import VectorAssembler
from pyspark.ml.regression import GBTRegressor
from pyspark.ml.evaluation import RegressionEvaluator
from pyspark.ml import Pipeline
import shutil
import os

# Chemins
SILVER_DATA_PATH = "/opt/airflow/data/silver"
MODEL_PATH = "/opt/airflow/models/eta_gbt_pipeline"
TEMP_MODEL_PATH = "/tmp/eta_gbt_pipeline_temp" # Dossier temporaire interne

def main():
    # 1. Init Spark avec configuration anti-permission
    spark = SparkSession.builder \
        .appName("Train_ETA_Model") \
        .config("spark.hadoop.fs.file.impl.disable.cache", "true") \
        .getOrCreate()

    spark.sparkContext.setLogLevel("WARN")
    
    print(f"Chargement des donn√©es Silver depuis {SILVER_DATA_PATH}...")
    try:
        df = spark.read.parquet(SILVER_DATA_PATH)
    except Exception as e:
        print(f"Erreur critique : Impossible de lire les donn√©es. {e}")
        return

    # 2. Pr√©paration des donn√©es
    print("Nettoyage des anomalies...")
    df_clean = df.filter(df.duration_minutes > 0).dropna()

    # Feature Engineering simple
    feature_cols = ["passenger_count", "trip_distance", "pickuphour", "dayof_week", "month", "payment_type"]
    assembler = VectorAssembler(inputCols=feature_cols, outputCol="features")
    
    # Split Train/Test
    train_data, test_data = df_clean.randomSplit([0.8, 0.2], seed=42)
    print(f"Donn√©es d'entra√Ænement : {train_data.count()} lignes")
    print(f"Donn√©es de test : {test_data.count()} lignes")

    # 3. Pipeline GBT (Gradient Boosted Trees)
    gbt = GBTRegressor(featuresCol="features", labelCol="duration_minutes", maxIter=10)
    pipeline = Pipeline(stages=[assembler, gbt])

    print("üèãÔ∏è Entra√Ænement du mod√®le GBT en cours...")
    model = pipeline.fit(train_data)

    # 4. √âvaluation
    print("√âvaluation du mod√®le...")
    predictions = model.transform(test_data)
    evaluator = RegressionEvaluator(labelCol="duration_minutes", predictionCol="prediction", metricName="rmse")
    rmse = evaluator.evaluate(predictions)
    
    r2_evaluator = RegressionEvaluator(labelCol="duration_minutes", predictionCol="prediction", metricName="r2")
    r2 = r2_evaluator.evaluate(predictions)

    print("R√©sultats sur le Test Set :")
    print(f"RMSE : {rmse:.4f}")
    print(f"R¬≤   : {r2:.4f}")

    # 5. Sauvegarde S√©curis√©e (Workaround Docker/Windows)
    print(f"üíæ Sauvegarde temporaire dans : {TEMP_MODEL_PATH}")
    
    # A. √âcriture dans le dossier temporaire Linux (pas de conflit de droits)
    if os.path.exists(TEMP_MODEL_PATH):
        shutil.rmtree(TEMP_MODEL_PATH)
    model.write().overwrite().save(TEMP_MODEL_PATH)

    # B. Copie vers le volume partag√© SANS les m√©tadonn√©es de permissions
    print(f"üì¶ D√©placement vers le dossier final : {MODEL_PATH}")
    if os.path.exists(MODEL_PATH):
        shutil.rmtree(MODEL_PATH)

    def copy_content_only(src, dst):
        shutil.copyfile(src, dst)

    try:
        shutil.copytree(TEMP_MODEL_PATH, MODEL_PATH, copy_function=copy_content_only)
        print("‚úÖ Mod√®le sauvegard√© avec succ√®s (Permissions contourn√©es).")
    except Exception as e:
        print(f"‚ö†Ô∏è Erreur lors de la copie finale : {e}")

    spark.stop()

if __name__ == "__main__":
    main()